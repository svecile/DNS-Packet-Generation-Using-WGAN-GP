{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation,Conv2DTranspose\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from functools import partial\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dnslib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def encode(dataset):\n",
    "    d = 2\n",
    "    w = 16\n",
    "    encodedData = []\n",
    "\n",
    "    for packet in dataset:\n",
    "        converted = []\n",
    "\n",
    "        for num in packet:\n",
    "            converted.append(int((int(num, base=16) * w) + (w / 2)))\n",
    "\n",
    "        matrix = np.zeros((28, 28))\n",
    "        r, c = 0, 0\n",
    "\n",
    "        for digit in converted:\n",
    "            matrix[r][c] = digit\n",
    "            matrix[r][c + 1] = digit\n",
    "            matrix[r + 1][c] = digit\n",
    "            matrix[r + 1][c + 1] = digit\n",
    "\n",
    "            if (c + 1) % 27 == 0:\n",
    "                c = 0\n",
    "                r += d\n",
    "            else:\n",
    "                c += d\n",
    "            encodedData.append(matrix)\n",
    "    return encodedData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def decode(encodedMatrix):\n",
    "\n",
    "    d = 2\n",
    "    w = 16\n",
    "    decodedData = []\n",
    "    r, c = 0, 0\n",
    "\n",
    "\n",
    "    avgArray = []\n",
    "\n",
    "    for _ in range(int((28 * 28) / 4)):\n",
    "        avgArray.append((encodedMatrix[r][c] + encodedMatrix[r][c + 1] + encodedMatrix[r + 1][c] +\n",
    "                         encodedMatrix[r + 1][c + 1]) / 4)\n",
    "\n",
    "        if (c + 1) % 27 == 0:\n",
    "            c = 0\n",
    "            r += d\n",
    "        else:\n",
    "            c += d\n",
    "\n",
    "    decodedArray = []\n",
    "\n",
    "    for val in avgArray:\n",
    "        byte = hex(int(val))\n",
    "        high = int(byte[2:3], 16)\n",
    "\n",
    "        if high <= 15:\n",
    "            low = 0\n",
    "        else:\n",
    "            low = int(byte[3:4], 16)\n",
    "\n",
    "        decodedArray.append(hex(int(((high / w) * w) + (low / w)))[2:])\n",
    "\n",
    "    decodedData.append(decodedArray)\n",
    "\n",
    "    return decodedArray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 loaded\n",
      "Dataset 2 loaded\n",
      "Dataset 3 loaded\n",
      "Dataset 4 loaded\n",
      "Dataset 5 loaded\n",
      "Dataset 6 loaded\n",
      "All datasets loaded\n",
      "Dataset encoded\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "#raw packet capture file\n",
    "'''\n",
    "with open(\"dns_samples_5480.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    # extract hex streams\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 1 loaded\")\n",
    "'''\n",
    "with open(\"dns_samples_6593.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 2 loaded\")\n",
    "\n",
    "with open(\"dns_samples_7224.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 3 loaded\")\n",
    "\n",
    "with open(\"dns_samples_7486.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 4 loaded\")\n",
    "\n",
    "#split up dataset encoding so dont run out of memory\n",
    "X_train1 = np.array(encode(dataset))\n",
    "# Rescale -1 to 1\n",
    "X_train1 = (X_train1.astype(np.float32) - 127.5) / 127.5\n",
    "dataset=[]\n",
    "\n",
    "with open(\"ping_800.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 5 loaded\")\n",
    "\n",
    "with open(\"dns_samples_7900.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"Dataset 6 loaded\")\n",
    "\n",
    "with open(\"dns_samples_13068.json\") as f:\n",
    "    frames_json = json.load(f)\n",
    "    hexStreams = [frame[\"_source\"][\"layers\"][\"frame_raw\"][0] for frame in frames_json]\n",
    "\n",
    "    for stream in hexStreams:\n",
    "        if len(stream)<=196:\n",
    "            hexArray = [stream[i:i + 1] for i in range(28, len(stream), 1)]\n",
    "            dataset.append(hexArray)\n",
    "print(\"All datasets loaded\")\n",
    "\n",
    "X_train = np.array(encode(dataset))\n",
    "print(\"Dataset encoded\")\n",
    "# Rescale -1 to 1\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.concatenate((X_train1, X_train))\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "dataset=[]\n",
    "X_train1=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def printPacket(packet, epoch):\n",
    "    print(\"IP Version: \", packet[0])\n",
    "    print(\"Header Length: \", packet[1])\n",
    "    print(\"Differentiated Services Field: \", hex(int((str(packet[2])+str(packet[3])), 16)))\n",
    "    print(\"Total Length: \", int((str(packet[4])+str(packet[5])+str(packet[6])+str(packet[7])), 16))\n",
    "    print(\"Identification: \", hex(int((str(packet[8])+str(packet[9])+str(packet[10])+str(packet[11])), 16)))\n",
    "    print(\"Flags: \", hex(int((str(packet[12])+str(packet[13])), 16)))\n",
    "    print(\"Fragment Offset: \", hex(int((str(packet[14])+str(packet[15])), 16)))\n",
    "    print(\"Time to live: \", int((str(packet[16])+str(packet[17])), 16))\n",
    "    print(\"Protocol: \", int((str(packet[18])+str(packet[19])), 16))\n",
    "    print(\"Header checksum: \", hex(int((str(packet[20])+str(packet[21])+str(packet[22])+str(packet[23])), 16)))\n",
    "    print(\"Source: \", (str(int((str(packet[24])+str(packet[25])), 16)))+\".\"+str(int((str(packet[26])+str(packet[27])), 16))+\".\"+str(int((str(packet[28])+str(packet[29])), 16))+\".\"+str(int((str(packet[30])+str(packet[31])), 16)))\n",
    "    print(\"Destination: \",(str(int((str(packet[32])+str(packet[33])), 16)))+\".\"+str(int((str(packet[34])+str(packet[35])), 16))+\".\"+str(int((str(packet[36])+str(packet[37])), 16))+\".\"+str(int((str(packet[38])+str(packet[39])), 16)))\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Source Port: \", int((str(packet[40])+str(packet[41])+str(packet[42])+str(packet[43])), 16))\n",
    "    print(\"Destination Port: \", int((str(packet[44])+str(packet[45])+str(packet[46])+str(packet[47])), 16))\n",
    "    print(\"Length: \", int((str(packet[48])+str(packet[49])+str(packet[50])+str(packet[51])), 16))\n",
    "    print(\"Checksum: \", hex(int((str(packet[52])+str(packet[53])+str(packet[54])+str(packet[55])), 16)))\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    try:\n",
    "        packet[0]=4\n",
    "        packet[14]=0\n",
    "        packet[15]=0\n",
    "        #make checksum 0\n",
    "        packet[52]=0\n",
    "        packet[53]=0\n",
    "        packet[54]=0\n",
    "        packet[55]=0\n",
    "        dns_string=[]\n",
    "        sep=''\n",
    "        for i in range(56, len(packet)):\n",
    "            dns_string.append(packet[i])\n",
    "        dns_string = sep.join(dns_string)\n",
    "        p = dnslib.binascii.unhexlify(dns_string)\n",
    "        d = dnslib.DNSRecord.parse(p)\n",
    "        print(d)\n",
    "\n",
    "        wgan.generator.save(\"GoodModels/generator-gp%d\" % epoch)\n",
    "        wgan.critic.save(\"GoodModels/critic-gp%d\" % epoch)\n",
    "    except:\n",
    "        print(\"DNS Malformed\")\n",
    "        print(\"----------------------------------\")\n",
    "        print(\"Transaction ID \", hex(int((str(packet[56])+str(packet[57])+str(packet[58])+str(packet[59])), 16)))\n",
    "        print(\"Flags: \", hex(int((str(packet[60])+str(packet[61])+str(packet[62])+str(packet[63])), 16)))\n",
    "        print(\"Questions: \", int((str(packet[64])+str(packet[65])+str(packet[66])+str(packet[67])), 16))\n",
    "        print(\"Answer RR's: \", int((str(packet[68])+str(packet[69])+str(packet[70])+str(packet[71])), 16))\n",
    "        print(\"Authority RR's: \", int((str(packet[72])+str(packet[73])+str(packet[74])+str(packet[75])), 16))\n",
    "        print(\"Additional RR's: \", int((str(packet[76])+str(packet[77])+str(packet[78])+str(packet[79])), 16))\n",
    "\n",
    "        #print(\"Hostname: \", bytes.fromhex(dns_string).decode('utf-8'))\n",
    "        #print(\"Type: \", int((str(packet[len(packet)-8])+str(packet[len(packet)-7])+str(packet[len(packet)-6])+str(packet[len(packet)-5])), 16))\n",
    "        #print(\"Class: \", int((str(packet[len(packet)-4])+str(packet[len(packet)-3])+str(packet[len(packet)-2])+str(packet[len(packet)-1])), 16))\n",
    "\n",
    "        dns_string=[]\n",
    "        sep=''\n",
    "        for i in range(80, len(packet)):\n",
    "            dns_string.append(packet[i])\n",
    "        dns_string = sep.join(dns_string)\n",
    "        print(dns_string)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 64\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.5)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(noise)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        BATCH_SIZE=32\n",
    "        alpha = K.random_uniform(shape = [BATCH_SIZE, 28, 28, 1], minval=0, maxval=1.0)\n",
    "        interpolated_img = real_img * alpha + (fake_img * (1-alpha))\n",
    "\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, noise],\n",
    "                                outputs=[valid, fake, validity_interpolated])\n",
    "\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss, self.wasserstein_loss, partial_gp_loss],\n",
    "                                            optimizer=optimizer,\n",
    "                                            loss_weights=[1, 1, 10])\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = 10 * K.square(gradient_l2_norm-1)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, use_bias=False, input_shape=(64,), kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Dense(1024, use_bias=False, input_shape=(64,), kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Dense(12544, use_bias=False, input_shape=(1024,), kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "        model.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Conv2D(1, (4, 4), strides=1, padding='same', use_bias=False))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.summary()\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=4, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.000025)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=50):\n",
    "\n",
    "        # real img class labels\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        #fake image class labels\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        loss =[]\n",
    "        loss1 = []\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.train_on_batch([imgs, noise], [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "            loss.append(d_loss[0])\n",
    "            loss1.append(d_loss[1])\n",
    "\n",
    "            if epoch%25==0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "            if epoch%100==0:\n",
    "                self.plotLoss(loss, loss1)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "            noise = np.random.normal(0, 1, (1, 64))\n",
    "            gen_pkt = self.generator.predict(noise)\n",
    "            gen_pkt = 127.5*gen_pkt[0,:,:,0]+127.5\n",
    "            gen_pkt=decode(np.array(gen_pkt).astype(\"int\"))\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"-------------------------------------------------\")\n",
    "            printPacket(gen_pkt, epoch)\n",
    "\n",
    "    def plotLoss(self, loss, loss1):\n",
    "        #clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(dpi = 120)\n",
    "        ax.set_ylabel(\"Critic Loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.plot(np.linspace(0, len(loss), len(loss)),loss, 'r')\n",
    "        ax.plot(np.linspace(0, len(loss1), len(loss1)),loss1, 'b')\n",
    "        plt.ylim([-100, 200])\n",
    "        plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training Started\")\n",
    "wgan = WGANGP()\n",
    "wgan.train(epochs=19000, batch_size=32, sample_interval=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "genModel = keras.models.load_model(\"GoodModels/generator-gp17800\", compile=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\spencer\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def genPkt(packet):\n",
    "\n",
    "        try:\n",
    "            packet[14]=0\n",
    "            packet[15]=0\n",
    "            #make checksum 0\n",
    "            packet[52]=0\n",
    "            packet[53]=0\n",
    "            packet[54]=0\n",
    "            packet[55]=0\n",
    "            #packet[0]=4\n",
    "            dns_string=[]\n",
    "            sep=''\n",
    "            for i in range(56, len(packet)):\n",
    "                dns_string.append(packet[i])\n",
    "            dns_string = sep.join(dns_string)\n",
    "            p = dnslib.binascii.unhexlify(dns_string)\n",
    "            d = dnslib.DNSRecord.parse(p)\n",
    "\n",
    "            print(\"--------------IP Header------------\")\n",
    "            print(\"IP Version: \", packet[0])\n",
    "            print(\"Header Length: \", packet[1])\n",
    "            print(\"Differentiated Services Field: \", hex(int((str(packet[2])+str(packet[3])), 16)))\n",
    "            print(\"Total Length: \", int((str(packet[4])+str(packet[5])+str(packet[6])+str(packet[7])), 16))\n",
    "            print(\"Identification: \", hex(int((str(packet[8])+str(packet[9])+str(packet[10])+str(packet[11])), 16)))\n",
    "            print(\"Flags: \", hex(int((str(packet[12])+str(packet[13])), 16)))\n",
    "            print(\"Fragment Offset: \", hex(int((str(packet[14])+str(packet[15])), 16)))\n",
    "            print(\"Time to live: \", int((str(packet[16])+str(packet[17])), 16))\n",
    "            print(\"Protocol: \", int((str(packet[18])+str(packet[19])), 16))\n",
    "            print(\"Header checksum: \", hex(int((str(packet[20])+str(packet[21])+str(packet[22])+str(packet[23])), 16)))\n",
    "            print(\"Source: \", (str(int((str(packet[24])+str(packet[25])), 16)))+\".\"+str(int((str(packet[26])+str(packet[27])), 16))+\".\"+str(int((str(packet[28])+str(packet[29])), 16))+\".\"+str(int((str(packet[30])+str(packet[31])), 16)))\n",
    "            print(\"Destination: \",(str(int((str(packet[32])+str(packet[33])), 16)))+\".\"+str(int((str(packet[34])+str(packet[35])), 16))+\".\"+str(int((str(packet[36])+str(packet[37])), 16))+\".\"+str(int((str(packet[38])+str(packet[39])), 16)))\n",
    "            print(\"---------------UDP Header-------------\")\n",
    "            print(\"Source Port: \", int((str(packet[40])+str(packet[41])+str(packet[42])+str(packet[43])), 16))\n",
    "            print(\"Destination Port: \", int((str(packet[44])+str(packet[45])+str(packet[46])+str(packet[47])), 16))\n",
    "            print(\"Length: \", int((str(packet[48])+str(packet[49])+str(packet[50])+str(packet[51])), 16))\n",
    "            print(\"Checksum: \", hex(int((str(packet[52])+str(packet[53])+str(packet[54])+str(packet[55])), 16)))\n",
    "            print(\"--------------DNS Packet--------------\")\n",
    "\n",
    "            print(d)\n",
    "            return 1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "count=0\n",
    "for _ in range(100):\n",
    "    #noise = np.random.normal(0, 1, (1, 64))\n",
    "    noise = np.random.uniform(0, 1, (1, 64))\n",
    "    gen_pkt = genModel.predict(noise)\n",
    "    gen_pkt = ((127.5*gen_pkt[0,:,:,0])+127.5).astype('int')\n",
    "    gen_pkt=decode(np.array(gen_pkt).astype(\"int\"))\n",
    "    count+=genPkt(gen_pkt)\n",
    "\n",
    "print(\"Success Rate: \", count/100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}